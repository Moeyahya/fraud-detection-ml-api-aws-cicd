{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f33fe67-2f70-47ad-bdf3-767bff0e8f03",
   "metadata": {},
   "source": [
    "## Fraud Detection API: Complete Project Documentation\n",
    "### From Local Development to Production CI/CD Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4f443-b2cd-49ec-9d24-029e654cc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px  # Interactive plots (optional)\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# EDA & profiling\n",
    "\n",
    "# Interactive EDA\n",
    "# import pandasgui\n",
    "# from pandasgui import show\n",
    "# show(your_dataframe)  # Opens an interactive GUI\n",
    "\n",
    "# Auto-EDA\n",
    "# from dataprep.eda import create_report\n",
    "# create_report(your_dataframe).show()\n",
    "\n",
    "# Lightweight Profiling \n",
    "import sweetviz as sv\n",
    "# sv.analyze(your_dataframe).show_html()\n",
    "\n",
    "# from pandas_profiling import ProfileReport  # Auto-EDA (install: `pip install ydata-profiling`)\n",
    "import missingno as msno  # Missing data visualization (install: `pip install missingno`)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "sns.set(style=\"darkgrid\",font_scale=1.5)\n",
    "pd.set_option(\"display.max.columns\",None)\n",
    "pd.set_option(\"display.max.rows\",None)\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress user warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress future warnings\n",
    "\n",
    "# Suppress specific warnings for LGBMClassifier and CatBoostClassifier\n",
    "import logging\n",
    "logging.getLogger(\"catboost\").setLevel(logging.ERROR)  # Suppress CatBoost logs\n",
    "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)  # Suppress LightGBM logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978e9c5-afa1-4b1d-a160-66bd51d4194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling/normalization\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    PowerTransformer,\n",
    "    label_binarize\n",
    ")\n",
    "\n",
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# Imputation (handling missing values)\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Outlier detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy import stats  # For Z-score, IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb652159-5a68-480d-a945-8048e38e0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic ML\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier,  # Faster alternative to CatBoost\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier  # Works on Python 3.13.2\n",
    "from lightgbm import LGBMClassifier  # Works on Python 3.13.2\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Neural Networks (optional)\n",
    "# import tensorflow as tf  # or `pip install tensorflow-cpu`\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d48a6-0c4f-4501-98c7-09c6c52dc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    mean_squared_error,\n",
    "    roc_curve\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6019752-e61a-4dce-811f-9ab4fb932ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Calibration\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Warnings (to clean output)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Time tracking\n",
    "import time\n",
    "from tqdm import tqdm  # Progress bars (install: `pip install tqdm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5e2c6-222b-44e3-8147-65afa4995d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_columns\", None)\n",
    "df = pd.read_csv(\"C:/Projects/Fraud Transaction Prediction/Fraud.csv\")# change this to your path\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50e116-1893-4bfa-8ea4-51a32f9fc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Calibration\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Warnings (to clean output)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Time tracking\n",
    "import time\n",
    "from tqdm import tqdm  # Progress bars (install: `pip install tqdm`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e33277-8f6f-4c04-a289-882511f60832",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f0cee-a8f1-439d-b23c-ed4cf2a767da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927b35f-5998-45f0-889f-604ffb2908b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f120e6-2896-4b7d-a5f1-3ce33d0bf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3c649-6e6a-455b-8909-77bad4a398e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df['step'].unique()\n",
    "print(unique_values) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7435233c-b1d0-469d-9309-51c727992eb9",
   "metadata": {},
   "source": [
    "#### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718cbdb8-82a9-48d1-b15b-b82b579eef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().to_frame().rename(columns={0:\"Total No. of Missing Values\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01856498-615f-4404-9ab6-a5c61570e43a",
   "metadata": {},
   "source": [
    "#### Checking for Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd9e8a-7334-41f0-bf51-3933f9687954",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate Values =\",df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343838a1-8857-4fed-97c7-d825e02c3705",
   "metadata": {},
   "source": [
    "#### Checking for numeric data columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8c50d-7349-47d3-8609-cec0649c4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = df.select_dtypes(include=[np.number])\n",
    "numeric_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f5973-4e9f-4cca-a062-217b74dffa42",
   "metadata": {},
   "source": [
    "#### Checking for categorical data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198831b-a119-4ee9-91fc-08dea8acd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select_dtypes('object').columns \n",
    "# or you can use below\n",
    "categorical_data = df.select_dtypes(exclude=[np.number])\n",
    "categorical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50732e5c-5cd9-4197-9bbb-d0d075dfc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_correlation_matrix(df, size=8):\n",
    "    \"\"\"\n",
    "    Plots a correlation matrix for numeric columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df : pandas.DataFrame\n",
    "    size : int - Output figure size\n",
    "    \"\"\"\n",
    "    # Select only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr = numeric_df.corr()\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(size, size))\n",
    "    \n",
    "    # Generate a heatmap\n",
    "    sns.heatmap(corr, \n",
    "                annot=True, \n",
    "                fmt=\".2f\", \n",
    "                cmap='coolwarm',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                linewidths=.5)\n",
    "    \n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bee9e4-9fc8-4b14-86fc-76cb20c61c10",
   "metadata": {},
   "source": [
    "##### it looks like we have categorical data that we need to take care of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e626c-2a93-4a15-92ce-e25d2062acfb",
   "metadata": {},
   "source": [
    "### further exploration and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3ce40-f4dd-4893-b246-c5ed4b95ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrelationMatrix(df1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ffac8-9c9d-4e1a-8230-3c015ea7d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea6db0-f89d-4e8e-a886-e6a86059b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(numeric_data.corr(),text_auto=True,aspect=\"auto\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf5a61-e1f5-4524-b733-24fcfc320f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8277ca-8719-435a-9929-19cd1e5f4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3d1f5-5600-4950-b535-59d90ed2fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126baff3-eea0-433a-abeb-f671f628f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15,10))\n",
    "plt.suptitle(\"Data Distribution\", fontsize=16)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233cf38-c597-4a02-b67b-f235c5bf290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='isFraud',data=df, palette='hls')\n",
    "plt.title(\"Imbalanced Fraud vs. Non-Fraud Distribution\", fontsize=14)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177de72-2a65-40f3-8006-58238e159c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df = df[['isFraud', 'amount']].groupby(['isFraud']).count()\n",
    "ratio_df.plot.pie(y='amount', figsize=(5, 5), labels=['0: nonfraud', '1: fraud'], autopct='%.1f%%', startangle=120, wedgeprops={'width': 0.75},title = 'Imbalanced Fraud vs. Non-Fraud Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3fd10-aafc-4c2a-9613-a70a1f11990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot\n",
    "df.boxplot(column=[\"step\",\"amount\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\"])\n",
    "plt.title(\"Box Plot Analysis\", fontsize=16)\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261560c8-7907-4176-8a9d-b263f6b6b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation test between the qualitative variable 'oldbalanceOrg' and the target variable\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 1. Point-Biserial Correlation\n",
    "corr, p_value = stats.pointbiserialr(df['isFraud'], df['oldbalanceOrg'])\n",
    "print(f\"Point-Biserial Correlation: {corr:.3f} (p-value: {p_value:.3e})\")\n",
    "\n",
    "# 2. Welch's t-test (unequal variances)\n",
    "fraud_bal = df.loc[df['isFraud'] == 1, 'oldbalanceOrg']\n",
    "nonfraud_bal = df.loc[df['isFraud'] == 0, 'oldbalanceOrg']\n",
    "t_stat, p_val = stats.ttest_ind(fraud_bal, nonfraud_bal, equal_var=False)\n",
    "print(f\"t-test: Mean difference = {fraud_bal.mean() - nonfraud_bal.mean():.1f} (p-value: {p_val:.3e})\")\n",
    "\n",
    "# 3. AUC-ROC Evaluation\n",
    "auc = roc_auc_score(df['isFraud'], df['oldbalanceOrg'])\n",
    "print(f\"AUC: {auc:.3f} (0.5 = random, 1 = perfect)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4638fae-6bb6-4e9c-8a4e-05bd662358d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_test=stats.f_oneway(df['oldbalanceOrg'][df['isFraud']==1],df['oldbalanceOrg'][df['isFraud']==0])\n",
    "print(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cde359-1b76-4872-9d0f-9586dd9db804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='isFraud', y='oldbalanceOrg', data=df, showfliers=False)\n",
    "plt.yscale('log')  # Use if data is highly skewed\n",
    "plt.title(\"Distribution of oldbalanceOrg by Fraud Status\")\n",
    "plt.xlabel(\"Is Fraud? (0=No, 1=Yes)\")\n",
    "plt.ylabel(\"Original Balance (log scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fdedc2-b1a2-42ce-b43a-9cc1288e823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze balance distribution for fraud vs non-fraud\n",
    "print(\"Fraudulent transactions balance percentiles:\")\n",
    "print(df[df['isFraud']==1]['oldbalanceOrg'].quantile([0.25, 0.5, 0.75, 0.9]))\n",
    "\n",
    "print(\"\\nNon-fraudulent transactions balance percentiles:\")\n",
    "print(df[df['isFraud']==0]['oldbalanceOrg'].quantile([0.25, 0.5, 0.75, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502f5cd-7059-425f-927c-1312ae5a7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentiles for fraud vs non-fraud transactions\n",
    "fraud_stats = df[df['isFraud']==1]['amount'].describe(percentiles=[.25, .5, .75, .9, .95, .99])\n",
    "nonfraud_stats = df[df['isFraud']==0]['amount'].describe(percentiles=[.25, .5, .75, .9, .95, .99])\n",
    "\n",
    "print(\"Fraudulent Transactions Amount Stats:\")\n",
    "print(fraud_stats[['min', '25%', '50%', '75%', '90%', '95%', '99%', 'max']])\n",
    "\n",
    "print(\"\\nNon-Fraudulent Transactions Amount Stats:\")\n",
    "print(nonfraud_stats[['min', '25%', '50%', '75%', '90%', '95%', '99%', 'max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f5ec4-a8b3-42a7-9d88-e009fdeedee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_thresholds(df, feature, is_fraud, upper_percentile=0.95, lower_percentile=0.05):\n",
    "    \"\"\"Calculate thresholds based on percentiles of fraud/non-fraud data\"\"\"\n",
    "    fraud_vals = df[df['isFraud']==is_fraud][feature]\n",
    "    return {\n",
    "        'upper': fraud_vals.quantile(upper_percentile),\n",
    "        'lower': fraud_vals.quantile(lower_percentile),\n",
    "        'mean': fraud_vals.mean()\n",
    "    }\n",
    "\n",
    "# Example usage for amount\n",
    "amount_thresholds = {\n",
    "    'fraud': get_dynamic_thresholds(df, 'amount', is_fraud=1),\n",
    "    'nonfraud': get_dynamic_thresholds(df, 'amount', is_fraud=0)\n",
    "}\n",
    "print(\"\\nAmount Thresholds:\")\n",
    "print(amount_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40dc73-0023-428a-8a6e-ac7edaa0952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set thresholds based on 95th percentile of non-fraud (adjust as needed)\n",
    "high_amount_thresh = nonfraud_stats['95%']\n",
    "balance_change_thresh = df[df['isFraud']==0]['oldbalanceOrg'].quantile(0.9)\n",
    "\n",
    "# Engineered features with dynamic thresholds\n",
    "df['high_amount_flag'] = (df['amount'] > high_amount_thresh).astype(int)\n",
    "df['suspicious_balance_change'] = (\n",
    "    (df['oldbalanceOrg'] - df['newbalanceOrig']) > balance_change_thresh\n",
    ").astype(int)\n",
    "\n",
    "# Ratio-based feature with smoothing\n",
    "df['amount_to_balance_ratio'] = df['amount'] / (df['oldbalanceOrg'] + 1)  # +1 prevents divide-by-zero\n",
    "\n",
    "# Time-sensitive features (if 'step' is in hours)\n",
    "# Set thresholds based on 95th percentile of non-fraud (adjust as needed)\n",
    "high_amount_thresh = nonfraud_stats['95%']\n",
    "balance_change_thresh = df[df['isFraud']==0]['oldbalanceOrg'].quantile(0.9)\n",
    "\n",
    "# Engineered features with dynamic thresholds\n",
    "df['high_amount_flag'] = (df['amount'] > high_amount_thresh).astype(int)\n",
    "df['suspicious_balance_change'] = (\n",
    "    (df['oldbalanceOrg'] - df['newbalanceOrig']) > balance_change_thresh\n",
    ").astype(int)\n",
    "\n",
    "# Ratio-based feature with smoothing\n",
    "df['amount_to_balance_ratio'] = df['amount'] / (df['oldbalanceOrg'] + 1)  # +1 prevents divide-by-zero\n",
    "\n",
    "# # Time features \n",
    "# # Feature 1: Hour of day (1-24, where 1 = 00:00-00:59 of any day)\n",
    "# df['hour_of_day'] = ((df['step'] - 1) % 24) + 1  # Converts to 1-24 range\n",
    "\n",
    "# # Feature 2: Day of simulation (1-31, since 743 hours ≈ 30.96 days)\n",
    "# df['day'] = ((df['step'] - 1) // 24) + 1  # 1-based day count\n",
    "\n",
    "# # Feature 3: Day of week (0=Monday to 6=Sunday)\n",
    "# df['day_of_week'] = ((df['step'] - 1) // 24) % 7  # 0-based weekday\n",
    "\n",
    "# # Feature 4: Weekend flag (1 if Saturday/Sunday)\n",
    "# df['is_weekend'] = ((df['day_of_week'] == 5) | (df['day_of_week'] == 6)).astype(int)\n",
    "\n",
    "# # Feature 5: Transaction burst (1 if same user transacts again within 1 hour)\n",
    "# df['txn_burst'] = (df.groupby('nameOrig')['step'].diff() == 1).astype(int)\n",
    "\n",
    "# # Feature 6: Hours since last transaction (NaN for first txn per user)\n",
    "# df['hours_since_last_txn'] = df.groupby('nameOrig')['step'].diff()\n",
    "\n",
    "# # Feature 7: Transactions per user in last 24 hours (rolling window)\n",
    "# df['txn_count_24h'] = df.groupby('nameOrig')['step'].transform(\n",
    "#     lambda x: x.rolling(24, min_periods=1).count()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71aaa48-4325-4fba-9c55-abce6c010f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fraud rates in new features\n",
    "print(\"\\nFraud Rates by Engineered Features:\")\n",
    "print(df.groupby('high_amount_flag')['isFraud'].mean())\n",
    "print(df.groupby('suspicious_balance_change')['isFraud'].mean())\n",
    "\n",
    "# Visual confirmation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "df.boxplot(column='amount_to_balance_ratio', by='isFraud', showfliers=False)\n",
    "plt.ylim(0, 5)  # Focus on 0-500% range\n",
    "plt.subplot(122)\n",
    "df.groupby('hour_of_day')['isFraud'].mean().plot()\n",
    "plt.title(\"Fraud Rate by Hour of Day\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c1ccf-0b0a-477c-bd75-38110ab6b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df, amount_percentile=0.95, balance_percentile=0.9):\n",
    "    \"\"\"Complete feature engineering with dynamic thresholds\"\"\"\n",
    "    # Calculate thresholds\n",
    "    amt_thresh = df[df['isFraud']==0]['amount'].quantile(amount_percentile)\n",
    "    bal_thresh = df[df['isFraud']==0]['oldbalanceOrg'].quantile(balance_percentile)\n",
    "    \n",
    "    # Transaction features\n",
    "    df['amount_to_balance'] = df['amount'] / (df['oldbalanceOrg'] + 1)\n",
    "    df['high_amount_flag'] = (df['amount'] > amt_thresh).astype(int)\n",
    "    \n",
    "    # Balance features\n",
    "    df['balance_change_abs'] = df['oldbalanceOrg'] - df['newbalanceOrig']\n",
    "    df['balance_change_ratio'] = df['balance_change_abs'] / (df['oldbalanceOrg'] + 1)\n",
    "    df['suspicious_withdrawal'] = (\n",
    "        (df['balance_change_abs'] > bal_thresh) & \n",
    "        (df['amount_to_balance'] > 0.5)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Type-specific features\n",
    "    df['large_cashout'] = (\n",
    "        (df['type'] == 'CASH_OUT') & \n",
    "        (df['amount_to_balance'] > 0.7)\n",
    "    ).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to your dataframe\n",
    "df = engineer_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce36eb7-2c71-40ac-94fb-8d22a5363059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time features \n",
    "# Feature 1: Hour of day (1-24, where 1 = 00:00-00:59 of any day)\n",
    "df['hour_of_day'] = ((df['step'] - 1) % 24) + 1  # Converts to 1-24 range\n",
    "\n",
    "# Feature 2: Day of simulation (1-31, since 743 hours ≈ 30.96 days)\n",
    "df['day'] = ((df['step'] - 1) // 24) + 1  # 1-based day count\n",
    "\n",
    "# Feature 3: Day of week (0=Monday to 6=Sunday)\n",
    "df['day_of_week'] = ((df['step'] - 1) // 24) % 7  # 0-based weekday\n",
    "\n",
    "# Feature 4: Weekend flag (1 if Saturday/Sunday)\n",
    "df['is_weekend'] = ((df['day_of_week'] == 5) | (df['day_of_week'] == 6)).astype(int)\n",
    "\n",
    "# Feature 5: Transaction burst (1 if same user transacts again within 1 hour)\n",
    "df['txn_burst'] = (df.groupby('nameOrig')['step'].diff() == 1).astype(int)\n",
    "\n",
    "# Feature 6: Hours since last transaction (NaN for first txn per user)\n",
    "df['hours_since_last_txn'] = df.groupby('nameOrig')['step'].diff()\n",
    "\n",
    "# Feature 7: Transactions per user in last 24 hours (rolling window)\n",
    "df['txn_count_24h'] = df.groupby('nameOrig')['step'].transform(\n",
    "    lambda x: x.rolling(24, min_periods=1).count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b381ba-602d-4f19-831a-f65243d402a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c3669-6fdb-40b0-a57c-d6604a0c892e",
   "metadata": {},
   "source": [
    "#### below features are just for exploration purposes, the purpose of this exercise is to learn run and keep it simple without going into extreme details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629e619-06cc-486c-aea5-63fe9ecd83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature engineering\n",
    "# # Set threshold at 90th percentile of non-fraud transactions\n",
    "# balance_threshold = df[df['isFraud']==0]['oldbalanceOrg'].quantile(0.9)\n",
    "# txn_ratio_threshold = 0.5  # 50% of balance\n",
    "\n",
    "# df['high_risk_balance'] = (df['oldbalanceOrg'] > balance_threshold).astype(int)\n",
    "# df['suspicious_withdrawal'] = (\n",
    "#     (df['oldbalanceOrg'] > balance_threshold) & \n",
    "#     (df['amount'] > txn_ratio_threshold * df['oldbalanceOrg'])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e64aa7-cb9a-493b-882e-718c507fc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flag transactions in top 5% of amounts\n",
    "# amount_threshold = df['amount'].quantile(0.95)\n",
    "# df['large_txn_flag'] = (df['amount'] > amount_threshold).astype(int)\n",
    "\n",
    "# # Combined flag\n",
    "# df['high_risk_combo'] = (\n",
    "#     df['high_risk_balance'] | \n",
    "#     df['large_txn_flag']\n",
    "# ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d84f5-0ff0-48cd-86d7-74e4fd350ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature Engineering \n",
    "# # High-risk balance threshold (adjust based on quartiles)  \n",
    "# df['high_risk_balance'] = (df['oldbalanceOrg'] > 1_000_000).astype(int)  \n",
    "\n",
    "# # Interaction with transaction amount  \n",
    "# df['large_balance_large_txn'] = (df['oldbalanceOrg'] > 500_000) & (df['amount'] > 0.9 * df['oldbalanceOrg'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca33bb-fff1-48c6-a8d2-28073978dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of all features we created \n",
    "# engineered_features = [\n",
    "#     'high_risk_balance',\n",
    "#     'large_balance_large_txn', \n",
    "#     'large_txn_flag',\n",
    "#     'high_risk_combo',\n",
    "#     'suspicious_withdrawal'\n",
    "# ]\n",
    "\n",
    "# # Safely remove columns\n",
    "# df = df.drop(columns=[col for col in engineered_features if col in df.columns], errors='ignore')\n",
    "\n",
    "# # Verify removal\n",
    "# print(\"Current columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b395c4-c085-4e15-bbf5-d442bbb46c10",
   "metadata": {},
   "source": [
    "#### First: Encode Categorical Variables\n",
    "(Convert text → numbers before handling imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a44cf-5151-4a70-a183-8ce2425d57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['type'], prefix='type') # handling the column 'type' first \n",
    "# Verify encoding\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2be48-96b2-477e-8f10-1d341ddf550f",
   "metadata": {},
   "source": [
    "#### Handling High-Cardinality ID Columns (nameOrig, nameDest)\n",
    "These columns appear to be transaction IDs (unique identifiers). Since they have extremely high cardinality (millions of unique values), do NOT one-hot encode them. Instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83e963-a9d4-4fa6-b82c-2c2e06ec74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['nameOrig', 'nameDest'], axis=1)  # Remove ID columns\n",
    "# Remove non-feature columns (including original categorical)\n",
    "X = df.drop(['isFraud', 'nameOrig', 'nameDest'], axis=1)\n",
    "y = df['isFraud']\n",
    "\n",
    "# Check all remaining features are numeric\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e21436-dda7-4491-a2cc-bed9bc35a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 100K rows for feature analysis\n",
    "sample_idx = np.random.choice(len(X), 100000, replace=False)\n",
    "X_sample = X.iloc[sample_idx]\n",
    "y_sample = y.iloc[sample_idx]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_sample, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a33d4-e0db-4ec1-907f-31cb366fb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=30,       # Reduced from 50\n",
    "    max_depth=5,           # Shallower trees\n",
    "    min_samples_leaf=100,  # Larger leaf nodes\n",
    "    n_jobs=-1,             # Use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X, y)  # Now runs much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4b6e3-2ff1-453d-8c29-8562b9c2e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster permutation importance (works with partial data)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    rf, X_sample, y_sample,\n",
    "    n_repeats=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "plt.barh(X.columns[sorted_idx][:15], result.importances_mean[sorted_idx][:15])\n",
    "plt.title(\"Permutation Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7b04c-960c-4a65-bc4f-e9cdc321d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(X, y)  # Now works with numeric-only data\n",
    "\n",
    "# Plot importance\n",
    "pd.Series(rf.feature_importances_, index=X.columns).nlargest(15).plot(kind='barh')\n",
    "plt.title(\"Top Predictive Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b5ac5-773d-4e83-b738-7a0000df266f",
   "metadata": {},
   "source": [
    "### Check if the data is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb40cea-e3ef-4b27-bf5f-f3ee6d9f248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_count = df['isFraud'].value_counts()\n",
    "fraud_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c35b67-c347-4b77-a04b-d71c872351ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_percentage = df['isFraud'].value_counts(normalize=True) * 100\n",
    "fraud_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0fa8bb-ff46-4e47-a276-0f862143d5fb",
   "metadata": {},
   "source": [
    "#### it looks like the data is highly imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8f88f-d011-4c06-be70-57f6495f6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135334e0-d6f1-41d5-83c4-9e05ce33d67a",
   "metadata": {},
   "source": [
    "#### First, Split Your Data (Critical!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d992df-e5d4-4473-8307-232f7da42854",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['isFraud'], axis = 1)\n",
    "y = df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41073c8d-ba93-4d16-ac15-aa5616178c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e83bd-a1ab-4f45-91a1-ed2a0f76061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train - >  \",x_train.shape)\n",
    "print(\"y_train - >  \",y_train.shape)\n",
    "print(\"x_test  - >  \",x_test.shape)\n",
    "print(\"y_test  - >  \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ccf9e-a854-48a8-ada2-1e13f6d085a2",
   "metadata": {},
   "source": [
    "#### Scale/transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6581ae1-46c0-4cf9-a3fe-f30736233def",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer(method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4dff7-5c2f-4e5e-9914-9f1a4e71306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = pt.fit_transform(x_train)\n",
    "x_test_scaled = pt.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8040a-011a-4ab4-85eb-a9a9acbc5113",
   "metadata": {},
   "source": [
    "#### handle Imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd78fbc-d2e2-4300-95a5-d150ccc1e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = {}\n",
    "# for i in df.select_dtypes('object').columns:\n",
    "#     encoder[i] = LabelEncoder()\n",
    "#     df[i] = encoder[i].fit_transform(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6d812-34f0-4f52-aaad-1be640b5b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.3, random_state=42)  # 3:10 fraud/non-fraud ratio\n",
    "X_res, y_res = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "print(\"Resampled class counts:\", y_res.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c607e0-e84f-4729-8338-96c461ff414d",
   "metadata": {},
   "source": [
    "### Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099c2e1-db92-4f69-84c2-c5cc17f6e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, name):\n",
    "    \"\"\"Train and evaluate a single model, returning metrics and plots.\"\"\"\n",
    "    try:\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "        \n",
    "        # Metrics\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Predicted Non-Fraud', 'Predicted Fraud'],\n",
    "                   yticklabels=['Actual Non-Fraud', 'Actual Fraud'])\n",
    "        plt.title(f'{name}\\nFP: {cm[0,1]} | FN: {cm[1,0]}')\n",
    "        plt.gca().add_patch(plt.Rectangle((1, 0), 1, 1, fill=False, edgecolor='red', lw=2))\n",
    "        plt.gca().add_patch(plt.Rectangle((0, 1), 1, 1, fill=False, edgecolor='orange', lw=2))\n",
    "        plt.show()\n",
    "        \n",
    "        # Return metrics\n",
    "        return {\n",
    "            'Model': name,\n",
    "            'Recall (Fraud)': report['1']['recall'],\n",
    "            'Precision (Fraud)': report['1']['precision'],\n",
    "            'F1 (Fraud)': report['1']['f1-score'],\n",
    "            'ROC AUC': roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "            'Type I (FP)': cm[0, 1],\n",
    "            'Type II (FN)': cm[1, 0]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed for {name}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a7b45-f8e3-4f19-a4e9-2c3ecf78dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []  \n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(\n",
    "    scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    tree_method='hist',  # Faster than exact\n",
    "    eval_metric='aucpr',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "results.append(evaluate_model(xgb, X_res, y_res, x_test_scaled, y_test, \"XGBoost (Fast)\"))\n",
    "del xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309e927-d31a-4be3-8df5-cfe0a7f0282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier(\n",
    "    class_weight='balanced',\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    feature_fraction=0.8,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "results.append(evaluate_model(lgbm, X_res, y_res, x_test_scaled, y_test, \"LightGBM\"))\n",
    "del lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123070ff-a5c1-4ea0-a20b-cff2f7c90b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Random Forest (Balanced)\n",
    "# ==============================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    n_estimators=50,\n",
    "    max_depth=7,\n",
    "    max_samples=0.8,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "results.append(evaluate_model(rf, X_res, y_res, x_test_scaled, y_test, \"Random Forest\"))\n",
    "del rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0853b-a61c-49a5-94dd-dacfacf9c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Gradient Boosting (Lightweight)\n",
    "# ==============================================\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "results.append(evaluate_model(gb, X_res, y_res, x_test_scaled, y_test, \"Gradient Boosting\"))\n",
    "del gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd1f9b-ae8f-4887-80ab-4dc7d028a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Logistic Regression (Fast)\n",
    "# ==============================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    solver='liblinear',\n",
    "    penalty='l1',\n",
    "    max_iter=200,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "results.append(evaluate_model(logreg, X_res, y_res, x_test_scaled, y_test, \"Logistic Reg\"))\n",
    "del logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499a1b3-970b-4ecd-a99e-ab927c303f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Support Vector Classifier - SVC (Caution)\n",
    "# # ==============================================\n",
    "# from sklearn.svm import SVC\n",
    "# svc = SVC(\n",
    "#     class_weight='balanced',\n",
    "#     probability=True,\n",
    "#     kernel='rbf',\n",
    "#     gamma='scale',\n",
    "#     random_state=42,\n",
    "#     cache_size=1000  # Helps with memory\n",
    "# )\n",
    "# svc_results = evaluate_model(svc, X_res, y_res, x_test_scaled, y_test, \"SVC\")\n",
    "# if svc_results: results.append(svc_results)\n",
    "# del svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be4046-2738-41b9-bcd6-35f6b6bcea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. AdaBoost (Quick)\n",
    "# ==============================================\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "results.append(evaluate_model(ada, X_res, y_res, x_test_scaled, y_test, \"AdaBoost\"))\n",
    "del ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747e313-d813-472c-9c78-fa71106d3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et = ExtraTreesClassifier(\n",
    "    class_weight='balanced',\n",
    "    n_estimators=50,\n",
    "    max_depth=7,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "results.append(evaluate_model(et, X_res, y_res, x_test_scaled, y_test, \"Extra Trees\"))\n",
    "del et"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531e329-5a34-4678-9835-75992111ee77",
   "metadata": {},
   "source": [
    "#### Combine and Rank Rsults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd4232-e34c-4361-a5af-117cba1d61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame - THIS MUST COME AFTER ALL MODELS RUN\n",
    "results_df = pd.DataFrame([r for r in results if r is not None])\n",
    "\n",
    "# Rank models by fraud detection performance\n",
    "ranked_df = results_df.sort_values([\n",
    "    'Recall (Fraud)', \n",
    "    'F1 (Fraud)',\n",
    "    'Type II (FN)'\n",
    "], ascending=[False, False, True])\n",
    "\n",
    "# Add rank column\n",
    "ranked_df['Rank'] = range(1, len(ranked_df)+1)\n",
    "\n",
    "# Highlight top 3 models\n",
    "def highlight_top3(s):\n",
    "    top3 = s.nlargest(3).index\n",
    "    return ['background-color: #FFFF00' if i in top3 else '' for i in range(len(s))]\n",
    "\n",
    "ranked_df.style.apply(highlight_top3, subset=['Recall (Fraud)', 'F1 (Fraud)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b49437b-be2d-4e35-9b7d-6aabcc888105",
   "metadata": {},
   "source": [
    "### Now that the model is complete here are the steps to Modularize the code and break it down as follows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6988f-84e2-4ebd-a4ea-1e78713d8ac2",
   "metadata": {},
   "source": [
    "### Step-1 Testing the Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861ead5-31fd-4161-9dd6-f89fdf255913",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_prediction/\n",
    "├── src/\n",
    "│   ├── config.py\n",
    "│   ├── preprocess.py\n",
    "│   ├── feature_engineer.py\n",
    "│   ├── predict.py\n",
    "│   ├── train.py\n",
    "│   ├── evaluate.py\n",
    "│   ├── app.py\n",
    "│   └── requirements.txt\n",
    "├── data/\n",
    "│   └── Fraud.csv\n",
    "├── models/ (will be created)\n",
    "└── logs/ (will be created)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb065d68-496e-4c6b-9a12-f9d897979922",
   "metadata": {},
   "source": [
    "#### Modularized Code Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99878bae-3d4c-4145-ae1c-76ab43648d19",
   "metadata": {},
   "source": [
    "##### 1- Config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1b2f8-9932-445d-b27d-401fa28f4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Project setup\n",
    "PROJECT_ROOT = Path(__file__).parent.parent\n",
    "DATA_PATH = PROJECT_ROOT / 'data' / 'Fraud.csv'\n",
    "MODEL_PATH = PROJECT_ROOT / 'models' / 'fraud_model.joblib'\n",
    "\n",
    "# Data loading\n",
    "N_ROWS = 100000 \n",
    "\n",
    "# Feature engineering\n",
    "AMOUNT_PERCENTILE = 0.95\n",
    "BALANCE_PERCENTILE = 0.9\n",
    "\n",
    "# Model training\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.3\n",
    "SMOTE_RATIO = 0.3\n",
    "\n",
    "class AppConfig:\n",
    "    # API Settings\n",
    "    HOST = \"0.0.0.0\"\n",
    "    PORT = 8080\n",
    "    DEBUG = False\n",
    "    \n",
    "    # Model Monitoring\n",
    "    PREDICTION_LOGS = PROJECT_ROOT / \"logs\" / \"predictions.log\"\n",
    "    DRIFT_THRESHOLD = 0.15\n",
    "\n",
    "    @classmethod\n",
    "    def ensure_dirs_exist(cls):\n",
    "        \"\"\"Create required directories\"\"\"\n",
    "        (PROJECT_ROOT / \"logs\").mkdir(exist_ok=True)\n",
    "        (PROJECT_ROOT / \"models\").mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize directories\n",
    "AppConfig.ensure_dirs_exist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363aa56-fa72-4890-8acf-7a0712a97c64",
   "metadata": {},
   "source": [
    "##### 2- Preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3cbce-b67b-4840-b556-fe6f924e5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from config import DATA_PATH, N_ROWS, TEST_SIZE, RANDOM_STATE, SMOTE_RATIO\n",
    "from feature_engineer import engineer_features\n",
    "import config  \n",
    "\n",
    "def load_and_preprocess():\n",
    "    \"\"\"Load data and apply preprocessing\"\"\"\n",
    "    df = pd.read_csv(DATA_PATH, nrows=N_ROWS)\n",
    "    df = engineer_features(df)\n",
    "    \n",
    "    X = df.drop(['isFraud', 'nameOrig', 'nameDest'], axis=1)\n",
    "    y = df['isFraud']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scaling\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    X_train_scaled = pt.fit_transform(X_train)\n",
    "    X_test_scaled = pt.transform(X_test)\n",
    "    \n",
    "    # Resampling\n",
    "    smote = SMOTE(sampling_strategy=SMOTE_RATIO, random_state=RANDOM_STATE)\n",
    "    X_res, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    return X_res, y_res, X_test_scaled, y_test, pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19112a-f149-4d3a-a3f3-aee9bdf763b5",
   "metadata": {},
   "source": [
    "##### 3- feature_engineer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61c531-7c3d-4d0e-8499-10934a987a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import AMOUNT_PERCENTILE, BALANCE_PERCENTILE\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Feature engineering pipeline\"\"\"\n",
    "    # Transaction features\n",
    "    amt_thresh = df[df['isFraud']==0]['amount'].quantile(AMOUNT_PERCENTILE)\n",
    "    bal_thresh = df[df['isFraud']==0]['oldbalanceOrg'].quantile(BALANCE_PERCENTILE)\n",
    "    \n",
    "    df['amount_to_balance'] = df['amount'] / (df['oldbalanceOrg'] + 1)\n",
    "    df['high_amount_flag'] = (df['amount'] > amt_thresh).astype(int)\n",
    "    df['balance_change_abs'] = df['oldbalanceOrg'] - df['newbalanceOrig']\n",
    "    df['suspicious_withdrawal'] = (\n",
    "        (df['balance_change_abs'] > bal_thresh) & \n",
    "        (df['amount_to_balance'] > 0.5)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Time features\n",
    "    df['hour_of_day'] = ((df['step'] - 1) % 24) + 1\n",
    "    df['day_of_week'] = ((df['step'] - 1) // 24) % 7\n",
    "    df['is_weekend'] = ((df['day_of_week'] == 5) | (df['day_of_week'] == 6)).astype(int)\n",
    "    \n",
    "    # Categorical encoding\n",
    "    df = pd.get_dummies(df, columns=['type'], prefix='type')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3959d-482c-4c62-846e-a113835aaa92",
   "metadata": {},
   "source": [
    "##### 4- Predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be84c85-595c-45e0-a99a-d31e66640420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict.py\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "from config import MODEL_PATH, AppConfig, AMOUNT_PERCENTILE, BALANCE_PERCENTILE \n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "class FraudPredictor:\n",
    "    def __init__(self):\n",
    "        self._load_model()\n",
    "        self._init_logging()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model artifacts with exact feature validation\"\"\"\n",
    "        try:\n",
    "            artifacts = load(MODEL_PATH)\n",
    "            self.model = artifacts['model']\n",
    "            self.pt = artifacts['transformer']\n",
    "            \n",
    "            # MUST match EXACTLY what was used in training\n",
    "            self.feature_order = [\n",
    "                'step', 'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
    "                'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud',\n",
    "                'amount_to_balance', 'high_amount_flag', 'balance_change_abs',\n",
    "                'suspicious_withdrawal', 'hour_of_day', 'day_of_week', 'is_weekend',\n",
    "                'type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER'\n",
    "            ]\n",
    "            print(\"✅ Model loaded with features:\", self.feature_order)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Model loading failed: {str(e)}\")\n",
    "\n",
    "    def _init_logging(self):\n",
    "        \"\"\"Set up prediction logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            filename=AppConfig.PREDICTION_LOGS,\n",
    "            format='%(asctime)s - %(message)s',\n",
    "            level=logging.INFO\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def _validate_input(self, data: dict) -> None:\n",
    "        \"\"\"Ensure minimum required fields exist\"\"\"\n",
    "        required_fields = {\n",
    "            'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
    "            'oldbalanceDest', 'newbalanceDest', 'step',\n",
    "            'isFlaggedFraud', 'type'\n",
    "        }\n",
    "        missing = required_fields - set(data.keys())\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required fields: {missing}\")\n",
    "\n",
    "    def log_prediction(self, data: dict, prediction: int):\n",
    "        \"\"\"Log prediction with context\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"input\": {k: v for k, v in data.items() if k != 'type'},  # Sanitize\n",
    "            \"prediction\": prediction,\n",
    "            \"model_version\": \"1.0.0\" \n",
    "        }\n",
    "        self.logger.info(json.dumps(log_entry))\n",
    "\n",
    "    def preprocess(self, transaction_data: dict):\n",
    "        \"\"\"Recreate features EXACTLY as during training\"\"\"\n",
    "        df = pd.DataFrame([transaction_data])\n",
    "        \n",
    "        # Feature engineering (MUST match training)\n",
    "        df['amount_to_balance'] = df['amount'] / (df['oldbalanceOrg'] + 1)\n",
    "        df['high_amount_flag'] = (df['amount'] > 10000).astype(int)\n",
    "        df['balance_change_abs'] = df['oldbalanceOrg'] - df['newbalanceOrig']\n",
    "        df['suspicious_withdrawal'] = (\n",
    "            (df['balance_change_abs'] > 5000) & \n",
    "            (df['amount_to_balance'] > 0.5)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Time features\n",
    "        df['hour_of_day'] = ((df['step'] - 1) % 24) + 1\n",
    "        df['day_of_week'] = ((df['step'] - 1) // 24) % 7\n",
    "        df['is_weekend'] = ((df['day_of_week'] == 5) | (df['day_of_week'] == 6)).astype(int)\n",
    "        \n",
    "        # Transaction type handling\n",
    "        valid_types = ['CASH_IN', 'CASH_OUT', 'DEBIT', 'PAYMENT', 'TRANSFER']\n",
    "        for t in valid_types:\n",
    "            df[f'type_{t}'] = 0\n",
    "        if 'type' in df and df['type'].iloc[0] in valid_types:\n",
    "            df[f'type_{df[\"type\"].iloc[0]}'] = 1\n",
    "        \n",
    "        # Verify feature match\n",
    "        missing = set(self.feature_order) - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing features after processing: {missing}\")\n",
    "            \n",
    "        return self.pt.transform(df[self.feature_order])\n",
    "\n",
    "    def predict(self, transaction_data: dict) -> int:\n",
    "        try:\n",
    "            self._validate_input(transaction_data)\n",
    "            processed = self.preprocess(transaction_data)\n",
    "            prediction = int(self.model.predict(processed)[0])\n",
    "            self.log_prediction(transaction_data, prediction)\n",
    "            return prediction\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Prediction failed: {str(e)}\")\n",
    "            raise RuntimeError(f\"Prediction failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887bd8f-a866-4a6c-aaa4-095d9fe6ff03",
   "metadata": {},
   "source": [
    "##### 5-Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b2c53-9cc0-4080-95d4-6cfcacf6ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import dump\n",
    "from preprocess import load_and_preprocess\n",
    "from config import MODEL_PATH, RANDOM_STATE\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd  # Added for feature order logging\n",
    "\n",
    "def train_model():\n",
    "    print(\"🚀 Starting model training...\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"🔍 Loading and preprocessing data...\")\n",
    "    X_res, y_res, X_test, y_test, pt = load_and_preprocess()\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"🤖 Initializing Random Forest model...\")\n",
    "    model = RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        n_estimators=50,\n",
    "        max_depth=7,\n",
    "        max_samples=0.8,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"⚡ Training model...\")\n",
    "    model.fit(X_res, y_res)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"🧪 Evaluating model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Log feature order used in training\n",
    "    print(\"\\n=== FEATURE ORDER USED IN TRAINING ===\")\n",
    "    if isinstance(X_res, pd.DataFrame):\n",
    "        print(X_res.columns.tolist())\n",
    "    else:\n",
    "        # If X_res is numpy array, we need to reconstruct feature names\n",
    "        # This assumes your load_and_preprocess() returns DataFrames\n",
    "        print(\"Warning: Features are numpy arrays - cannot display names\")\n",
    "    \n",
    "    # Save model\n",
    "    dump({'model': model, 'transformer': pt}, MODEL_PATH)\n",
    "    print(f\"\\n✅ Model successfully saved to {MODEL_PATH}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eda158-2944-4e64-94ee-6a234a29f3e4",
   "metadata": {},
   "source": [
    "##### 6-Evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b943408-8ec3-40ad-aa55-1b857526ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973f03b-bab0-4719-87ce-ff71e1e2c58e",
   "metadata": {},
   "source": [
    "#### Now Lets Build Flask API (app.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4da78a-4550-4722-8d21-7a0c122082fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from predict import FraudPredictor\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from config import AppConfig\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize predictor\n",
    "try:\n",
    "    predictor = FraudPredictor()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to initialize predictor: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data:\n",
    "            return jsonify({\"error\": \"No JSON provided\"}), 400\n",
    "            \n",
    "        # Add request metadata\n",
    "        request_meta = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"endpoint\": \"predict\",\n",
    "            \"client_ip\": request.remote_addr\n",
    "        }\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = predictor.predict(data)\n",
    "        \n",
    "        return jsonify({\n",
    "            \"fraud_prediction\": prediction,\n",
    "            \"meta\": request_meta,\n",
    "            \"model_info\": {\n",
    "                \"version\": \"1.0.0\",\n",
    "                \"type\": \"RandomForest\"\n",
    "            },\n",
    "            \"status\": \"success\"\n",
    "        })\n",
    "        \n",
    "    except ValueError as e:\n",
    "        return jsonify({\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"input_error\"\n",
    "        }), 400\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"server_error\"\n",
    "        }), 500\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\n",
    "        \"status\": \"healthy\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_loaded\": True\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=AppConfig.HOST, port=AppConfig.PORT, debug=AppConfig.DEBUG)\n",
    "\n",
    "print(\" * Application started successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d5a191-0c04-475b-9047-9b79cccc6413",
   "metadata": {},
   "source": [
    "#### Open your CLI in your folder location (in my case I am using cmd) and here are the steps along with the following commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8279db3-7e94-4e41-8ad2-2c03de77b91e",
   "metadata": {},
   "source": [
    "##### 1-Install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2fdf85-b76b-40fe-a049-be009ac740fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r src\\requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602add3e-22fa-41cd-8c27-5f20e44fa122",
   "metadata": {},
   "source": [
    "##### 2- Run the training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6d17c-f60b-4d4b-87dc-6e3ec7d41f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python src\\train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed68740-7838-407f-97aa-067b8a9e6896",
   "metadata": {},
   "source": [
    "##### 3- Start the API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8251dd5-ec35-4e23-960a-0aaf8f2a8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python src\\app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bbb8a7-b37c-4000-b89d-eca2318b9140",
   "metadata": {},
   "source": [
    "##### Expected output should look like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b12b55-9995-4cf9-ae8f-2fa3feaff5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Projects\\fraud_detection\\src>python app.py\n",
    "✅ Model loaded with features: ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud', 'amount_to_balance', 'high_amount_flag', 'balance_change_abs', 'suspicious_withdrawal', 'hour_of_day', 'day_of_week', 'is_weekend', 'type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER']\n",
    " * Serving Flask app 'app'\n",
    " * Debug mode: off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09945a55-d59c-4fc8-bfc8-1e162494f551",
   "metadata": {},
   "source": [
    "##### Now let's put this Model to Test! Copy the features in exact order and give it values. Now in a new CMD window execute the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfef1f1-971b-4f5b-9b23-e80c97faf552",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:8080/predict ^\n",
    "-H \"Content-Type: application/json\" ^\n",
    "-d \"{^\n",
    "\\\"step\\\": 1,^\n",
    "\\\"amount\\\": 9839.64,^\n",
    "\\\"oldbalanceOrg\\\": 170136.0,^\n",
    "\\\"newbalanceOrig\\\": 160296.36,^\n",
    "\\\"oldbalanceDest\\\": 0.0,^\n",
    "\\\"newbalanceDest\\\": 9839.64,^\n",
    "\\\"isFlaggedFraud\\\": 0,^\n",
    "\\\"type\\\": \\\"CASH_OUT\\\",^\n",
    "\\\"amount_to_balance\\\": 0.0578,^\n",
    "\\\"high_amount_flag\\\": 1,^\n",
    "\\\"balance_change_abs\\\": 9839.64,^\n",
    "\\\"suspicious_withdrawal\\\": 0,^\n",
    "\\\"hour_of_day\\\": 1,^\n",
    "\\\"day_of_week\\\": 0,^\n",
    "\\\"is_weekend\\\": 0^\n",
    "}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aebde14-fe40-4ea5-aec4-9a502c4e4dff",
   "metadata": {},
   "source": [
    "### Step-2 Dockerizing the app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb603ee-c4f4-40cc-83c7-3507a9914b54",
   "metadata": {},
   "source": [
    "#### Create Your serve.py file serves two main purposes:\n",
    "\n",
    "#### 1- Replaces Flask's Development Server\n",
    "\n",
    "##### Flask's built-in server (app.run()) is not suitable for production (slow, insecure, single-threaded).\n",
    "\n",
    "##### waitress is a production-ready WSGI server that handles multiple requests efficiently.\n",
    "\n",
    "#### 2- Standardizes the Startup Process\n",
    "\n",
    "##### Provides a consistent entry point for Docker to launch your app.\n",
    "\n",
    "##### Ensures directories exist and logging is configured before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea9228-4471-4ae8-a97e-e0241c786ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serve.py\n",
    "from waitress import serve\n",
    "from app import app  # Import your Flask app\n",
    "from config import PROJECT_ROOT\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Production configuration\n",
    "MODEL_DIR = PROJECT_ROOT / 'models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('waitress')\n",
    "logger.info('Starting server...')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(f\"🚀 Serving fraud detection API on http://localhost:8080\")\n",
    "    serve(app, host='0.0.0.0', port=8080)  # Production-ready server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b03467-96d0-412a-a6ce-204581baf0f9",
   "metadata": {},
   "source": [
    "#### Create Dockerfile in your project root. and makesure to remove .txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754276b-5538-487d-83eb-a559aca4d679",
   "metadata": {},
   "source": [
    "#### Core Purpose of the Dockerfile\n",
    "\n",
    "##### Your Dockerfile is a blueprint that:\n",
    "\n",
    "##### - Creates a reproducible environment for your app\n",
    "\n",
    "##### - Packages all dependencies, code, and models into a single deployable unit\n",
    "\n",
    "##### - Standardizes how your app runs across different machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f8dbb-c331-4d22-8d99-1938b2ac656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_detection/\n",
    "├── src/\n",
    "│   ├── app.py               # Flask app\n",
    "│   ├── config.py            # Config\n",
    "│   ├── ...                  # Other Python files\n",
    "│   ├── requirements.txt     # Dependencies\n",
    "    └── Dockerfile               # Your Dockerfile thatès where it would be created\n",
    "├── models/\n",
    "│   └── fraud_model.joblib   # Your trained model\n",
    "└── data/\n",
    "    └── Fraud.csv            # Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43e4cfe-3abb-4079-927e-da9de12cb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile\n",
    "FROM python:3.10-slim\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy model file (ensure it's in your project directory)\n",
    "COPY fraud_model.joblib /models/\n",
    "\n",
    "# Copy requirements first for caching\n",
    "COPY requirements.txt .\n",
    "RUN pip install --upgrade pip setuptools wheel\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the rest of the app\n",
    "COPY . .\n",
    "\n",
    "# Verify files (debugging)\n",
    "RUN ls -l /app/\n",
    "RUN ls -l /models/\n",
    "\n",
    "# Use Waitress for production WSGI server\n",
    "CMD [\"python\", \"-c\", \"from waitress import serve; from app import app; print('Launching app...'); serve(app, host='0.0.0.0', port=8080)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc4450-e06c-44d9-8266-72ab45b1532c",
   "metadata": {},
   "source": [
    "#### Now Open CMD in the folder location and execute this command to build the docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb37c6e-40f8-4630-b668-7ec5a9c821b7",
   "metadata": {},
   "source": [
    "#### 1. Local Docker Testing (Dev Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b6b31-6e7b-41e5-b511-8c61e3b527b2",
   "metadata": {},
   "source": [
    "##### Step 1: Build the Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdeb6b1-65f1-4205-9e62-d83d1ddf7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to your project directory\n",
    "# docker build --no-cache -t fraud-detection-api ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ca7e7-1091-44d1-976d-764024819cd7",
   "metadata": {},
   "source": [
    "##### Step 2: Verify the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03ddf92-4d55-4495-b3c7-1e95c5f8c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker images <-- execute after the built is successfully done it should return something like this \n",
    "C:\\Projects\\fraud_detection\\src>docker images\n",
    "REPOSITORY            TAG       IMAGE ID       CREATED         SIZE\n",
    "fraud-detection-api   latest    5e4c6603ae57   4 minutes ago   686MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfdb08-b29c-4410-8ede-aab321e08a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed commands\n",
    "docker ps -a  # List all containers\n",
    "docker stop <container-id>  # Stop the one using port\n",
    "docker rm <container-id>  # Remove it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a74946e-49fb-43b5-ae93-9d8b8ce472a2",
   "metadata": {},
   "source": [
    "##### Step 3: Run the Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400ef7d-73ea-4f4e-bd26-6cd8b4e103dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -it -p 8080:8080 ^\n",
    "  -v \"%cd%\\models\":/app/models ^\n",
    "  -v \"%cd%\\logs\":/app/logs ^\n",
    "  --name fraud-container ^\n",
    "  fraud-detection-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10f682-9917-45fb-a897-312f67638bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results should look somthing like this \n",
    "C:\\Projects\\fraud_detection\\src>docker run -it -p 8080:8080 ^\n",
    "More?   -v \"%cd%\\models\":/app/models ^\n",
    "More?   -v \"%cd%\\logs\":/app/logs ^\n",
    "More?   --name fraud-container ^\n",
    "More?   fraud-detection-api\n",
    "✅ Model loaded with features: ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud', 'amount_to_balance', 'high_amount_flag', 'balance_change_abs', 'suspicious_withdrawal', 'hour_of_day', 'day_of_week', 'is_weekend', 'type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER']\n",
    " * Application started successfully!\n",
    "Launching app...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a727cb-0b7d-4003-ab32-c2171d06e2fb",
   "metadata": {},
   "source": [
    "##### In a NEW CMD Window, verify it is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84059fe0-ffa8-4388-b562-34de46069cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61499503-7db7-4486-bd6c-f49caf341283",
   "metadata": {},
   "source": [
    "##### Step 4: Test the API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b41ab-c886-4796-91f1-0c48b8dc8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:8080/predict ^\n",
    "-H \"Content-Type: application/json\" ^\n",
    "-d \"{\\\"step\\\":1,\\\"amount\\\":1000,\\\"oldbalanceOrg\\\":5000,\\\"newbalanceOrig\\\":4000,\\\"oldbalanceDest\\\":0,\\\"newbalanceDest\\\":1000,\\\"isFlaggedFraud\\\":0,\\\"amount_to_balance\\\":0.2,\\\"high_amount_flag\\\":0,\\\"balance_change_abs\\\":1000,\\\"suspicious_withdrawal\\\":0,\\\"hour_of_day\\\":10,\\\"day_of_week\\\":2,\\\"is_weekend\\\":0,\\\"type\\\":\\\"CASH_OUT\\\",\\\"type_CASH_IN\\\":0,\\\"type_CASH_OUT\\\":1,\\\"type_DEBIT\\\":0,\\\"type_PAYMENT\\\":0,\\\"type_TRANSFER\\\":0}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205939e-d715-471a-8f64-5b8d7c4b29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Your output should look something like this \n",
    "C:\\Projects\\fraud_detection\\src>curl -X POST http://localhost:8080/predict ^\n",
    "More? -H \"Content-Type: application/json\" ^\n",
    "More? -d \"{\\\"step\\\":1,\\\"amount\\\":1000,\\\"oldbalanceOrg\\\":5000,\\\"newbalanceOrig\\\":4000,\\\"oldbalanceDest\\\":0,\\\"newbalanceDest\\\":1000,\\\"isFlaggedFraud\\\":0,\\\"amount_to_balance\\\":0.2,\\\"high_amount_flag\\\":0,\\\"balance_change_abs\\\":1000,\\\"suspicious_withdrawal\\\":0,\\\"hour_of_day\\\":10,\\\"day_of_week\\\":2,\\\"is_weekend\\\":0,\\\"type\\\":\\\"CASH_OUT\\\",\\\"type_CASH_IN\\\":0,\\\"type_CASH_OUT\\\":1,\\\"type_DEBIT\\\":0,\\\"type_PAYMENT\\\":0,\\\"type_TRANSFER\\\":0}\"\n",
    "{\"fraud_prediction\":0,\"meta\":{\"client_ip\":\"172.17.0.1\",\"endpoint\":\"predict\",\"timestamp\":\"2025-04-18T01:14:44.901829\"},\"model_info\":{\"type\":\"RandomForest\",\"version\":\"1.0.0\"},\"status\":\"success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b147c5d-360f-4104-be18-591c229c1c2d",
   "metadata": {},
   "source": [
    "##### View Real-Time Logs, Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9d0cd-ba6f-4b1f-8b34-73ce81c4a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker logs -f fraud-container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c634f-16aa-420f-a42e-9f449ad3de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Additional details In case you need to rest \n",
    "docker stop fraud-container\n",
    "docker rm fraud-container\n",
    "docker rmi fraud-detection-api\n",
    "docker build -t fraud-detection-api ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a2103-b7cf-400f-a9ce-7e798d2a9528",
   "metadata": {},
   "source": [
    "#### Transition to Cloud Deployment: The Model is Now Portable\n",
    "Now the fraud detection model is fully containerized and ready for deployment to any cloud platform (AWS, Azure, GCP) or on-premises Kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643394e2-d7c1-4cb4-af9c-9fb3350e17ab",
   "metadata": {},
   "source": [
    "##### By Dockerizing the model, I’ve achieved:\n",
    "✅ Reproducibility: Anyone with my Docker image can run the exact same environment (dependencies, code, and configurations).\n",
    "\n",
    "✅ Portability: The same image runs identically on any cloud or local machine.\n",
    "\n",
    "✅ Scalability: Ready to deploy on Kubernetes for high availability and load balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02779d8-ecf8-43ba-bfca-793b4b0f639e",
   "metadata": {},
   "source": [
    "#### 2- Local Kubernetes (Docker Desktop)\n",
    "this will mimic how it would work if you deploy it using any of the cloud platforms (AWS, GCP, AZURE) using kubernetes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e42e3d-7d0f-48a0-a6ef-1c786659cc71",
   "metadata": {},
   "source": [
    "#### Step: 1 In the folder location, Create YAML files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde63577-3648-435d-83ee-8f376b764d03",
   "metadata": {},
   "source": [
    "#### deployement.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5da80-cdcb-4a2d-b118-34e4cefe94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: fraud-detection-api\n",
    "spec:\n",
    "  replicas: 1  # Single replica for local testing\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: fraud-detection\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: fraud-detection\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: fraud-api\n",
    "        image: fraud-detection-api:latest  # Uses local Docker image\n",
    "        ports:\n",
    "        - containerPort: 8080\n",
    "        volumeMounts:\n",
    "        - mountPath: /app/models\n",
    "          name: models-volume\n",
    "        - mountPath: /app/logs\n",
    "          name: logs-volume\n",
    "      volumes:\n",
    "      - name: models-volume\n",
    "        hostPath:\n",
    "          path: /run/desktop/mnt/host/c/Projects/fraud_detection/src/models  # WSL 2 host path\n",
    "          type: Directory\n",
    "      - name: logs-volume\n",
    "        hostPath:\n",
    "          path: /run/desktop/mnt/host/c/Projects/fraud_detection/src/logs\n",
    "          type: Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db8a26-0121-41cb-ab30-ddb1bdc529c0",
   "metadata": {},
   "source": [
    "#### service.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9558c-8d77-4d28-b2e5-860dd56a0bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: fraud-detection-service\n",
    "spec:\n",
    "  type: NodePort  # For local access\n",
    "  selector:\n",
    "    app: fraud-detection\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8080\n",
    "      nodePort: 30080  # Fixed port for easy access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6d8fa-d28c-4d66-965b-d59b2a173128",
   "metadata": {},
   "source": [
    "#### Step 2: Enable Kubernetes \n",
    "1- Open Docker Desktop → Settings → Kubernetes → Enable.\n",
    "\n",
    "2- Verify:\n",
    "\n",
    "cmd\n",
    "\n",
    "kubectl cluster-info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299d1e3-b298-410d-bc34-d5f08f4b5a26",
   "metadata": {},
   "source": [
    "#### Step 3: Deploy to Local Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1d6c5-86ee-48ad-9557-9448c71d9365",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apply Configurations\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl apply -f service.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daad51a-0370-4933-8ebd-7639ea71c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "Verify Deployement\n",
    "kubectl get pods -w  # Watch pod status (should transition to \"Running\")\n",
    "NAME                                   READY   STATUS    RESTARTS   AGE\n",
    "fraud-detection-api-5f7d8c6c58-abcde   1/1     Running   0          30s\n",
    "kubectl get svc      # Check service details\n",
    "NAME                      TYPE       CLUSTER-IP      PORT(S)        AGE\n",
    "fraud-detection-service   NodePort   10.96.XXX.XXX   80:30080/TCP   XXs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7e950-666e-4c47-9db3-249b67f81476",
   "metadata": {},
   "outputs": [],
   "source": [
    "Make a prediction request\n",
    "in CMD Window execute\n",
    "curl -X POST http://localhost:30080/predict ^\n",
    "-H \"Content-Type: application/json\" ^\n",
    "-d \"{\\\"step\\\":1, \\\"amount\\\":1000, \\\"oldbalanceOrg\\\":5000, \\\"newbalanceOrig\\\":4000, \\\"oldbalanceDest\\\":1000, \\\"newbalanceDest\\\":2000, \\\"isFlaggedFraud\\\":0, \\\"type\\\":\\\"TRANSFER\\\"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc5cf6-6e51-4e7b-ac39-a83f6d066d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alternative way test fraud detection cases (use poershell as it is more reliable for JSON)\n",
    "$fraudTest = {\n",
    "    $body = @{\n",
    "        step = 1\n",
    "        amount = 999999\n",
    "        oldbalanceOrg = 5000\n",
    "        newbalanceOrig = 0\n",
    "        oldbalanceDest = 0\n",
    "        newbalanceDest = 999999\n",
    "        isFlaggedFraud = 1\n",
    "        type = \"TRANSFER\"\n",
    "    } | ConvertTo-Json\n",
    "    Invoke-RestMethod -Uri \"http://localhost:30080/predict\" -Method Post -Body $body -ContentType \"application/json\"\n",
    "}\n",
    "\n",
    "1..5 | ForEach-Object { Start-Job -ScriptBlock $fraudTest } | Wait-Job | Receive-Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575ffcb-ffa5-4b2f-bb41-a4897c2bc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel Testing Command (PowerShell)\n",
    "# 1. Create a test function\n",
    "$testRequest = {\n",
    "    $body = @{\n",
    "        step = 1\n",
    "        amount = 1000\n",
    "        oldbalanceOrg = 5000\n",
    "        newbalanceOrig = 4000\n",
    "        oldbalanceDest = 1000\n",
    "        newbalanceDest = 2000\n",
    "        isFlaggedFraud = 0\n",
    "        type = \"TRANSFER\"\n",
    "    } | ConvertTo-Json\n",
    "\n",
    "    try {\n",
    "        $response = Invoke-RestMethod -Uri \"http://localhost:30080/predict\" -Method Post -Body $body -ContentType \"application/json\"\n",
    "        [PSCustomObject]@{\n",
    "            Success = $true\n",
    "            Prediction = $response.fraud_prediction\n",
    "            Timestamp = $response.meta.timestamp\n",
    "        }\n",
    "    } catch {\n",
    "        [PSCustomObject]@{\n",
    "            Success = $false\n",
    "            Error = $_.Exception.Message\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Run 10 parallel requests\n",
    "$jobs = 1..10 | ForEach-Object {\n",
    "    Start-Job -ScriptBlock $testRequest\n",
    "}\n",
    "\n",
    "# 3. Wait for completion and show results\n",
    "$results = $jobs | Wait-Job | Receive-Job\n",
    "$jobs | Remove-Job\n",
    "\n",
    "# 4. Display formatted results\n",
    "$results | Format-Table -AutoSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbae79-22b7-4913-a968-68ec5cd304c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Add Monitoring (In powershell)\n",
    "# Install Kubernetes metrics server (if not installed)\n",
    "kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n",
    "\n",
    "# Set up dashboard\n",
    "kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n",
    "kubectl proxy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7ae3a-647b-4d1a-9ab7-245dcd03bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Access at: \n",
    "http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35fb86-b4c5-4e5e-8fab-7169b5dc8165",
   "metadata": {},
   "source": [
    "### Stage 4: Building a Complete CI/CD Pipeline for the Fraud detection API using GitHub Actions, Docker, and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffa6fb-684d-4730-a3f5-00d080998999",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "Before we begin, ensure you have:\n",
    "\n",
    "1- Git installed on your local machine\n",
    "\n",
    "2- Docker installed\n",
    "\n",
    "3- Kubernetes (via Docker Desktop) enabled\n",
    "\n",
    "4- A GitHub account\n",
    "\n",
    "5- Your code ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c6479-aca4-4d89-b88b-22db40b7af7b",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize Your Local Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5dd0c-6d1f-47e1-aa2e-b7687d40a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory for your project (if starting fresh)\n",
    "mkdir fraud-detection-cicd\n",
    "cd fraud-detection-cicd\n",
    "\n",
    "# Initialize git repository\n",
    "git init\n",
    "\n",
    "# Create basic directory structure\n",
    "mkdir -p src/config src/preprocess src/feature_engineer src/train src/predict src/evaluate\n",
    "mkdir tests deployments\n",
    "\n",
    "# Create README, sample below \n",
    "echo \"# Fraud Detection ML API with CI/CD Pipeline\" > README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56da30-f25c-4a07-9a05-1115b961ed33",
   "metadata": {},
   "source": [
    "#### Step 2: Add Your Existing Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a02cd9-962a-424a-9e45-0b806229d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud-detection-cicd/\n",
    "├── src/\n",
    "│   ├── config/\n",
    "│   ├── preprocess/\n",
    "│   ├── feature_engineer/\n",
    "│   ├── train/\n",
    "│   ├── predict/\n",
    "│   ├── evaluate/\n",
    "│   └── app.py (your Flask app)\n",
    "├── tests/\n",
    "├── deployments/\n",
    "│   ├── deployment.yaml\n",
    "│   └── service.yaml\n",
    "├── Dockerfile\n",
    "├── requirements.txt\n",
    "└── README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622d2b7-4406-489e-b562-11b2065115c4",
   "metadata": {},
   "source": [
    "#### Step 3: Create a .gitignore File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5879a2-ac0b-4878-a3a7-0fccbd341c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .gitignore file\n",
    "echo \"venv/\n",
    "*.pyc\n",
    "__pycache__/\n",
    "*.swp\n",
    ".env\n",
    "*.pkl\n",
    "*.model\n",
    ".DS_Store\n",
    ".ipynb_checkpoints/\n",
    "*.egg-info/\n",
    "dist/\n",
    "build/\" > .gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e93c56-3412-4d42-8a77-c2a67e70f6e1",
   "metadata": {},
   "source": [
    "#### Step 4: Connect to GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43806deb-7135-413c-be52-7264eb5cb2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage all files\n",
    "git add .\n",
    "\n",
    "# Authorize \n",
    "\n",
    "git config --global user.email \"your_email@example.com\"\n",
    "git config --global user.name \"Your Name\"\n",
    "\n",
    "# Commit initial files\n",
    "git commit -m \"Initial commit with basic structure\"\n",
    "\n",
    "# Verify the remote was added correctly\n",
    "git remote -v\n",
    "\n",
    "should show:\n",
    "origin  https://github.com/moeyahya/your_repository.git (fetch)\n",
    "origin  https://github.com/moeyahya/your_repository.git (push)\n",
    "\n",
    "# Then connect your local repository to GitHub (if not connected)\n",
    "git remote add origin https://github.com/moeyahya/fraud-detection-ml-api-aws-cicd.git\n",
    "\n",
    "# Push your code\n",
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186a70c-04f9-4d5e-8563-66954082986a",
   "metadata": {},
   "source": [
    "#### Step 5: Set Up GitHub Actions for CI/CD\n",
    "mkdir -p .github/workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be73cef-487b-406a-aaef-4b98e9556244",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create a CI Workflow File\n",
    "Create a file named .github/workflows/ci.yml with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11b25b-aea3-429e-a2b2-c75ff4ca7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: Continuous Integration\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ \"main\" ]\n",
    "  pull_request:\n",
    "    branches: [ \"main\" ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    timeout-minutes: 10\n",
    "\n",
    "    steps:\n",
    "    - uses: actions/checkout@v4\n",
    "    \n",
    "    - name: Set up Python 3.9\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "        cache: 'pip'\n",
    "\n",
    "    - name: Create dummy model directory\n",
    "      run: |\n",
    "        mkdir -p models\n",
    "        echo \"Dummy model directory for CI tests\" > models/README.md\n",
    "\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -e .\n",
    "        pip install pytest pytest-cov\n",
    "\n",
    "    - name: Run tests\n",
    "      run: |\n",
    "        pytest tests/ \\\n",
    "          --cov=src \\\n",
    "          --cov-report=xml \\\n",
    "          --cov-report=term-missing \\\n",
    "          -v\n",
    "      env:\n",
    "        PYTHONPATH: ${{ github.workspace }}\n",
    "        MODEL_PATH: \"./models/nonexistent.joblib\"\n",
    "\n",
    "    - name: Upload coverage\n",
    "      uses: codecov/codecov-action@v3\n",
    "      with:\n",
    "        token: ${{ secrets.CODECOV_TOKEN }}\n",
    "        files: coverage.xml\n",
    "        flags: unittests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e74bff-bb95-4d74-a90d-a467f461319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create a CD Workflow File\n",
    "Create a file named .github/workflows/cd.yml with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e357706-a2a6-4917-810c-b80c8dd465ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: Continuous Deployment\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ \"main\" ]\n",
    "    paths:\n",
    "      - 'src/**'\n",
    "      - 'Dockerfile'\n",
    "      - 'requirements.txt'\n",
    "      - 'deployments/**'\n",
    "\n",
    "jobs:\n",
    "  build-and-deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    environment: production\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Log in to Docker Hub\n",
    "      uses: docker/login-action@v2\n",
    "      with:\n",
    "        username: ${{ secrets.DOCKER_HUB_USERNAME }}\n",
    "        password: ${{ secrets.DOCKER_HUB_TOKEN }}\n",
    "    \n",
    "    - name: Build and push Docker image\n",
    "      run: |\n",
    "        docker build -t moeyahya/fraud-detection-api:latest .\n",
    "        docker push moeyahya/fraud-detection-api:latest\n",
    "        \n",
    "    - name: Install kubectl\n",
    "      uses: azure/setup-kubectl@v3\n",
    "      \n",
    "    - name: Deploy to Kubernetes\n",
    "      run: |\n",
    "        echo \"${{ secrets.KUBE_CONFIG }}\" > kubeconfig.yaml\n",
    "        export KUBECONFIG=kubeconfig.yaml\n",
    "        \n",
    "        kubectl apply -f deployments/deployment.yaml\n",
    "        kubectl apply -f deployments/service.yaml\n",
    "        \n",
    "        kubectl rollout status deployment/fraud-detection-deployment\n",
    "        kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20831d-4d00-42a9-b587-59b7cd006012",
   "metadata": {},
   "source": [
    "#### Step 6: Set Up GitHub Secrets\n",
    "For the CD pipeline to work, you need to set up secrets in your GitHub repository:\n",
    "\n",
    "1- Go to your GitHub repository\n",
    "\n",
    "2- Click on \"Settings\" > \"Secrets and variables\" > \"Actions\"\n",
    "\n",
    "3- Click \"New repository secret\"\n",
    "\n",
    "Add these secrets:\n",
    "\n",
    "-DOCKER_HUB_USERNAME: Your Docker Hub username\n",
    "\n",
    "-DOCKER_HUB_TOKEN: Your Docker Hub access token (create in Docker Hub account settings)\n",
    "\n",
    "-KUBE_CONFIG: Your Kubernetes config file content (get this from ~/.kube/config on your local machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b5f28-0358-4e0d-891e-67ec0eb2d228",
   "metadata": {},
   "source": [
    "##### 🚀 Your CI/CD pipeline should now authenticate with Docker Hub and Kubernetes automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33213f6c-4934-441e-bf34-9084791364ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add __init__.py Files\n",
    "Create these empty files to make Python treat directories as packages:\n",
    "\n",
    "powershell\n",
    "# In your project root:\n",
    "New-Item -ItemType File src/__init__.py\n",
    "New-Item -ItemType File tests/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d726bc1-a2be-438d-a466-acd1ed304b97",
   "metadata": {},
   "source": [
    "#### Ensure your project looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6248c0-82dd-4a04-b745-5030c87dbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud-detection-ml-api-aws-cicd/\n",
    "├── .github/\n",
    "│   └── workflows/\n",
    "│       ├── ci.yml\n",
    "│       └── cd.yml\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── app.py\n",
    "│   └── ... (other source files)\n",
    "├── tests/\n",
    "│   ├── __init__.py\n",
    "│   └── test_app.py\n",
    "├── setup.py\n",
    "└── requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af9d071-c87c-4c49-b373-c5306d3249fb",
   "metadata": {},
   "source": [
    "#### Step 7: Add Tests (Optional but Recommended)\n",
    "test_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf85806-9100-4c8e-b3d0-8a70456e7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from src.app import app\n",
    "\n",
    "class TestAPI(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        app.config['TESTING'] = True\n",
    "        self.client = app.test_client()\n",
    "    \n",
    "    def test_health_check(self):\n",
    "        response = self.client.get('/health')\n",
    "        self.assertEqual(response.status_code, 200)\n",
    "        self.assertEqual(response.json['status'], 'healthy')\n",
    "        self.assertIn('model_loaded', response.json)\n",
    "\n",
    "    def test_predict_endpoint(self):\n",
    "        test_data = {\n",
    "            \"amount\": 100,\n",
    "            \"oldbalanceOrg\": 1000,\n",
    "            \"newbalanceOrig\": 900,\n",
    "            \"oldbalanceDest\": 500,\n",
    "            \"newbalanceDest\": 600,\n",
    "            \"step\": 1,\n",
    "            \"isFlaggedFraud\": 0,\n",
    "            \"type\": \"TRANSFER\"\n",
    "        }\n",
    "        response = self.client.post('/predict', json=test_data)\n",
    "        self.assertEqual(response.status_code, 200)\n",
    "        self.assertIn('fraud_prediction', response.json)\n",
    "        self.assertEqual(response.json['model_info']['test_mode'], True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a799ee3d-82b4-4dfe-a5a6-edaaf615b439",
   "metadata": {},
   "source": [
    "#### Create setup.py (Recommended)\n",
    "Create this file in your project root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f4cd9-a620-41c8-bc78-1988f925633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"fraud-detection\",\n",
    "    version=\"0.1\",\n",
    "    packages=find_packages(include=['src*']), \n",
    "    install_requires=[\n",
    "        'pandas>=1.5.0',\n",
    "        'scikit-learn>=1.2.0',\n",
    "        'Flask>=2.0.0',\n",
    "        'joblib>=1.0.0',\n",
    "        'imbalanced-learn>=0.10.0',\n",
    "        'scipy>=1.7.0',\n",
    "        'numpy>=1.21.0',\n",
    "        'waitress>=2.1.0'      ## from your requirements file\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44192c-3bb9-4b95-b7de-83ecd42e7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Correct any imports in your files accordingly \n",
    "for example\n",
    "# In src/app.py:\n",
    "from src.predict import FraudPredictor\n",
    "\n",
    "# In src/predict.py:\n",
    "from src.app import some_helper_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa05b54-06c2-449d-ba71-573e0253dc60",
   "metadata": {},
   "source": [
    "##### for example src/predict.py and src/app.py need to be modified with correct source imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d653b2-c4bb-4e5c-8352-101809965125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict.py\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "from src.config import MODEL_PATH, AppConfig, AMOUNT_PERCENTILE, BALANCE_PERCENTILE \n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "class FraudPredictor:\n",
    "    def __init__(self, test_mode=False):\n",
    "        self.test_mode = test_mode\n",
    "        self.model = None\n",
    "        self.pt = None\n",
    "        self.feature_order = []\n",
    "        self._init_logging()\n",
    "        if not test_mode:\n",
    "            self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model artifacts with exact feature validation\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(MODEL_PATH):\n",
    "                raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n",
    "                \n",
    "            artifacts = load(MODEL_PATH)\n",
    "            self.model = artifacts['model']\n",
    "            self.pt = artifacts['transformer']\n",
    "            \n",
    "            self.feature_order = [\n",
    "                'step', 'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
    "                'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud',\n",
    "                'amount_to_balance', 'high_amount_flag', 'balance_change_abs',\n",
    "                'suspicious_withdrawal', 'hour_of_day', 'day_of_week', 'is_weekend',\n",
    "                'type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER'\n",
    "            ]\n",
    "            print(\"✅ Model loaded with features:\", self.feature_order)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Model loading failed: {str(e)}\")\n",
    "            if not self.test_mode:\n",
    "                raise RuntimeError(f\"Model loading failed: {str(e)}\")\n",
    "\n",
    "    def _init_logging(self):\n",
    "        \"\"\"Set up prediction logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            filename=AppConfig.PREDICTION_LOGS,\n",
    "            format='%(asctime)s - %(message)s',\n",
    "            level=logging.INFO\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def _validate_input(self, data: dict) -> None:\n",
    "        \"\"\"Ensure minimum required fields exist\"\"\"\n",
    "        required_fields = {\n",
    "            'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
    "            'oldbalanceDest', 'newbalanceDest', 'step',\n",
    "            'isFlaggedFraud', 'type'\n",
    "        }\n",
    "        missing = required_fields - set(data.keys())\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required fields: {missing}\")\n",
    "\n",
    "    def log_prediction(self, data: dict, prediction: int):\n",
    "        \"\"\"Log prediction with context\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"input\": {k: v for k, v in data.items() if k != 'type'},\n",
    "            \"prediction\": prediction,\n",
    "            \"model_version\": \"1.0.0\" \n",
    "        }\n",
    "        self.logger.info(json.dumps(log_entry))\n",
    "\n",
    "    def preprocess(self, transaction_data: dict):\n",
    "        \"\"\"Recreate features EXACTLY as during training\"\"\"\n",
    "        df = pd.DataFrame([transaction_data])\n",
    "        \n",
    "        # Feature engineering\n",
    "        df['amount_to_balance'] = df['amount'] / (df['oldbalanceOrg'] + 1)\n",
    "        df['high_amount_flag'] = (df['amount'] > 10000).astype(int)\n",
    "        df['balance_change_abs'] = df['oldbalanceOrg'] - df['newbalanceOrig']\n",
    "        df['suspicious_withdrawal'] = (\n",
    "            (df['balance_change_abs'] > 5000) & \n",
    "            (df['amount_to_balance'] > 0.5)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Time features\n",
    "        df['hour_of_day'] = ((df['step'] - 1) % 24) + 1\n",
    "        df['day_of_week'] = ((df['step'] - 1) // 24) % 7\n",
    "        df['is_weekend'] = ((df['day_of_week'] == 5) | (df['day_of_week'] == 6)).astype(int)\n",
    "        \n",
    "        # Transaction type handling\n",
    "        valid_types = ['CASH_IN', 'CASH_OUT', 'DEBIT', 'PAYMENT', 'TRANSFER']\n",
    "        for t in valid_types:\n",
    "            df[f'type_{t}'] = 0\n",
    "        if 'type' in df and df['type'].iloc[0] in valid_types:\n",
    "            df[f'type_{df[\"type\"].iloc[0]}'] = 1\n",
    "            \n",
    "        # Verify feature match\n",
    "        missing = set(self.feature_order) - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing features after processing: {missing}\")\n",
    "            \n",
    "        return self.pt.transform(df[self.feature_order])\n",
    "\n",
    "    def predict(self, transaction_data: dict) -> int:\n",
    "        if self.test_mode:\n",
    "            return 0  # Dummy prediction in test mode\n",
    "            \n",
    "        try:\n",
    "            self._validate_input(transaction_data)\n",
    "            processed = self.preprocess(transaction_data)\n",
    "            prediction = int(self.model.predict(processed)[0])\n",
    "            self.log_prediction(transaction_data, prediction)\n",
    "            return prediction\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Prediction failed: {str(e)}\")\n",
    "            raise RuntimeError(f\"Prediction failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a9c8a-66d8-4de8-93d6-a9d66dfb2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/app.py\n",
    "from flask import Flask, request, jsonify\n",
    "from src.predict import FraudPredictor\n",
    "from datetime import datetime\n",
    "from src.config import AppConfig\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize predictor with test mode if MODEL_PATH doesn't exist\n",
    "try:\n",
    "    predictor = FraudPredictor(test_mode=not os.path.exists('models/fraud_model.joblib'))\n",
    "    print(\"ℹ️ Predictor initialized in test mode\" if predictor.test_mode else \"✅ Predictor initialized with model\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to initialize predictor: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data:\n",
    "            return jsonify({\"error\": \"No JSON provided\"}), 400\n",
    "            \n",
    "        request_meta = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"endpoint\": \"predict\",\n",
    "            \"client_ip\": request.remote_addr\n",
    "        }\n",
    "        \n",
    "        prediction = predictor.predict(data)\n",
    "        \n",
    "        return jsonify({\n",
    "            \"fraud_prediction\": prediction,\n",
    "            \"meta\": request_meta,\n",
    "            \"model_info\": {\n",
    "                \"version\": \"1.0.0\",\n",
    "                \"type\": \"RandomForest\",\n",
    "                \"test_mode\": predictor.test_mode\n",
    "            },\n",
    "            \"status\": \"success\"\n",
    "        })\n",
    "        \n",
    "    except ValueError as e:\n",
    "        return jsonify({\"error\": str(e), \"status\": \"input_error\"}), 400\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e), \"status\": \"server_error\"}), 500\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\n",
    "        \"status\": \"healthy\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_loaded\": not predictor.test_mode\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=AppConfig.HOST, port=AppConfig.PORT, debug=AppConfig.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774b490-6324-4b1c-b56c-c87c95161e57",
   "metadata": {},
   "source": [
    "#### Step 8: Commit and Push Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3ee31-1e1b-4e93-8a99-061ea1b87a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "git add .\n",
    "git commit -m \"Add CI/CD workflows and basic tests\"\n",
    "git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfddf45-c9a5-4e18-9b1d-2ba6b1fd0fde",
   "metadata": {},
   "source": [
    "#### Step 9: Monitor the Workflows\n",
    "1- Go to your GitHub repository\n",
    "\n",
    "2- Click on the \"Actions\" tab\n",
    "\n",
    "3- You should see your workflows running\n",
    "\n",
    "4- Click on each workflow to see detailed logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58671409-f2b6-4733-8d4c-8cb53a15a6f2",
   "metadata": {},
   "source": [
    "#### Step 10: Verify Deployment\n",
    "Once the CD pipeline completes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4373cdb-01c7-46ca-abb0-acd774338324",
   "metadata": {},
   "outputs": [],
   "source": [
    "1- Check your Kubernetes cluster:\n",
    "\n",
    "powershell\n",
    "kubectl get pods\n",
    "kubectl get services\n",
    "\n",
    "PS C:\\Projects\\fraud-detection-cicd> kubectl get pods\n",
    "NAME                                   READY   STATUS    RESTARTS   AGE\n",
    "fraud-detection-api-7875d449bb-xl6tz   1/1     Running   0          2d3h\n",
    "test-api-5f8664f4cb-9nqd7              1/1     Running   0          2d6h\n",
    "\n",
    "PS C:\\Projects\\fraud-detection-cicd> kubectl get services\n",
    "NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\n",
    "fraud-detection-service   NodePort    10.102.207.162   <none>        80:30080/TCP   3d\n",
    "kubernetes                ClusterIP   10.96.0.1        <none>        443/TCP        3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2dc3c-6434-425c-80c7-fb66d10e0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "2- Find the external IP (if using LoadBalancer) or port-forward to access your service:\n",
    "\n",
    "powershell\n",
    "kubectl port-forward service/fraud-detection-service 5000:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad35726-cda3-415b-9d19-ae0048481946",
   "metadata": {},
   "outputs": [],
   "source": [
    "3- Test the API endpoint:\n",
    "\n",
    "powershell\n",
    "curl http://localhost:5000/health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984baa38-ed4e-49c6-8b7f-95442624e6c5",
   "metadata": {},
   "source": [
    "CI Pipeline:\n",
    "\n",
    "✅ Runs on every push\n",
    "\n",
    "✅ Creates sample data\n",
    "\n",
    "✅ Runs tests\n",
    "\n",
    "✅ Uploads coverage\n",
    "\n",
    "CD Pipeline:\n",
    "\n",
    "✅ Builds Docker image only when relevant paths change (src/, Dockerfile, etc.)\n",
    "\n",
    "✅ Pushes to Docker Hub\n",
    "\n",
    "✅ Conditionally deploys to Kubernetes (if KUBE_CONFIG exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8cba60-a2d5-4ca2-b842-4438f17e2195",
   "metadata": {},
   "source": [
    "#### Step 4: Access the Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055ca83-8726-458d-bffd-7726cc6a78e7",
   "metadata": {},
   "source": [
    "#### 2. On-Prem Kubernetes Deployment (Staging/Prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551769f9-9a28-47c3-be1b-23ce4ac58f9a",
   "metadata": {},
   "source": [
    "#### Push Image to Private Registry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6418c-e226-429c-a898-678267aadf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Authenticate with AWS ECR\n",
    "aws ecr get-login-password | docker login --username AWS --password-stdin YOUR_ID_HERE.dkr.ecr.ca-central-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ae2c4-79bd-4deb-b779-b3325e5081fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tag Image\n",
    "docker tag fraud-detection-api:latest YOUR_ID_HERE.dkr.ecr.ca-central-1.amazonaws.com/fraud-detection-api:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef75fc-d601-49cc-ab20-994f975c9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "Push Image\n",
    "docker push YOUR_ID_HERE.dkr.ecr.ca-central-1.amazonaws.com/fraud-detection-api:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839e209-8113-4e40-96ea-acae69f0b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "Verify the push was successful\n",
    "aws ecr list-images --repository-name fraud-detection-api --region ca-central-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040b6e7-de06-4f97-8f16-41bbc8b8a674",
   "metadata": {},
   "source": [
    "## Detailed Plan: Deploying Fraud Detection Model on AWS with CI/CD Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be2172-cdf8-4951-9cb7-e06b7c756adb",
   "metadata": {},
   "source": [
    "### 📌 Phase 1: AWS Free Tier Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d2dc7-8c42-4dbd-973e-fe701217d513",
   "metadata": {},
   "source": [
    "### Step 1: Create an AWS Account & Set Up CLI\n",
    "##### 1- Sign up for AWS Free Tier (aws.amazon.com/free) . make sure to save your access key ID and Secret access key as they will be used when configuring aws\n",
    "\n",
    "##### * Avoid services that aren’t free (check pricing before deploying).\n",
    "\n",
    "##### 2- Install AWS CLI (AWS CLI Install Guide)\n",
    "\n",
    "##### * Verify installation:\n",
    "##### aws --version\n",
    "##### 3- Configure AWS CLI\n",
    "##### aws configure\n",
    "##### Enter your Access Key ID, Secret Access Key, Default Region (us-east-1), and Output Format (json).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fead526-3efe-4d96-80ba-f8382316a7ad",
   "metadata": {},
   "source": [
    "### Step 2: Set Up IAM (Identity & Access Management)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549e25b-69bd-49e7-bb57-5d3b05dc8058",
   "metadata": {},
   "source": [
    "#### 1- Go to IAM Console → Users → Add User\n",
    "\n",
    "##### Name: fraud-detection-deployer\n",
    "\n",
    "#### Under Permissions in the search bar add:\n",
    "\n",
    "##### AmazonEC2ContainerRegistryFullAccess\n",
    "\n",
    "##### AmazonECS_FullAccess (if using ECS)\n",
    "\n",
    "##### AWSCodePipeline_FullAccess\n",
    "\n",
    "##### AWSCodeBuildAdminAccess\n",
    "\n",
    "##### AWSCloudFormationFullAccess (optional, for Infrastructure as Code)\n",
    "#### 2- Generate Access Keys (for CI/CD later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec7a6b-f207-492d-86a8-160648ba377a",
   "metadata": {},
   "source": [
    "### 📌 Phase 2: Docker & AWS ECR (Elastic Container Registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb97ecf-febe-4908-8c36-1dd87303f8be",
   "metadata": {},
   "source": [
    "#### Step 1: Push Docker Image to AWS ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e255a-5a13-495f-9824-f4c713a5e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "1- Login to AWS ECR\n",
    "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin YOUR_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com\n",
    "\n",
    "To get your AWS account ID and other identity info \n",
    "aws sts get-caller-identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d426d2a-7613-44de-bb92-739af6e476dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "2- Create a Repository\n",
    "aws ecr create-repository --repository-name fraud-detection --region us-east-1 (you can add whatever region you are working from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eee41d-c2c4-48e0-a500-c4f6bae38c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "3- Tag & Push Docker Image\n",
    "docker tag fraud-detection:latest YOUR_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/fraud-detection:latest\n",
    "docker push YOUR_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/fraud-detection:latest\n",
    "\n",
    "On your AWS Console under fraud-detection-deployer (user created earlier) check for image to ensure image is attached \n",
    "or you can execute \n",
    "aws ec2 describe-images --owners self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d4eb4-1f8a-43f3-b190-08dedbd6bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "4- Push to AWS ECR:\n",
    "docker push 311410995726.dkr.ecr.us-east-1.amazonaws.com/fraud-detection:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f7d9c-4002-4651-b10e-73c1f0a93269",
   "metadata": {},
   "source": [
    "### 📌 Phase 3: Deploying the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9695b4b6-426f-4b92-ba5f-d2b10b22e1a6",
   "metadata": {},
   "source": [
    "#### Option A: Deploying with AWS ECS (Elastic Container Service)\n",
    "ECS is a container orchestration service that runs Docker containers. We’ll use Fargate (serverless) to avoid managing servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0a4c1-c856-42d2-b463-473f489bf17c",
   "metadata": {},
   "source": [
    "#### Step 1: Create a Task Definition (JSON Config for Container)\n",
    "A Task Definition tells ECS how to run your Docker container.\n",
    "\n",
    "#### 1- Go to AWS ECS Console:\n",
    "\n",
    " - Open AWS ECS Console\n",
    "\n",
    " - Click Task Definitions → Create new Task Definition\n",
    "\n",
    "#### 2- Select Launch Type:\n",
    "\n",
    " - Choose Fargate (serverless) → Click Next Step\n",
    "\n",
    "#### 3- Configure Task Definition:\n",
    "\n",
    " - Task Definition Name: fraud-detection-task\n",
    "\n",
    " - Task Role: None (for now, unless you need AWS permissions)\n",
    "\n",
    " - Network Mode: awsvpc (required for Fargate)\n",
    "\n",
    " - Task Execution Role:\n",
    "\n",
    "If none exists, click Create new role (AWS will auto-generate one).\n",
    "\n",
    "#### 4- Set Task Size (Free Tier Limits):\n",
    "\n",
    " - CPU: 0.25 vCPU\n",
    "\n",
    " - Memory: 0.5 GB\n",
    "\n",
    "#### 5- Add Container:\n",
    "\n",
    " - Click Add Container\n",
    "\n",
    " - Container Name: fraud-detection-container\n",
    "\n",
    " - Image: Paste your ECR image URI (e.g., 311410995726.dkr.ecr.us-east-1.amazonaws.com/fraud-detection:latest)\n",
    "\n",
    "Port Mappings:\n",
    "\n",
    " - If your app runs on port 5000 (Flask default), add:\n",
    "\n",
    "   Container Port: 5000\n",
    "\n",
    "   Protocol: TCP\n",
    "\n",
    "#### Click Add → Create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42735e8-293b-4faa-a178-901714f57b9d",
   "metadata": {},
   "source": [
    "### Step 2: Create an ECS Cluster (Fargate, Serverless)\n",
    "A Cluster is a logical group of tasks/services.\n",
    "\n",
    "Go to ECS Console → Clusters → Create Cluster\n",
    "\n",
    "Select Template:\n",
    "\n",
    "Choose \"Networking only (Fargate)\" → Click Next Step\n",
    "\n",
    "Configure Cluster:\n",
    "\n",
    "Cluster Name: fraud-detection-cluster\n",
    "\n",
    "Leave VPC & Subnets as default (AWS picks for you)\n",
    "\n",
    "Click Create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9fc7e8-c1e4-4be4-9362-53ee6f8087d8",
   "metadata": {},
   "source": [
    "#### Step 3: Create an ECS Service (Runs Your Task)\n",
    "A Service ensures your task keeps running (auto-restarts if crashes).\n",
    "\n",
    "Inside your Cluster → Click Create Service\n",
    "\n",
    "Configure Service:\n",
    "\n",
    "Launch Type: FARGATE\n",
    "\n",
    "Task Definition: Select fraud-detection-task (latest revision)\n",
    "\n",
    "Cluster: fraud-detection-cluster\n",
    "\n",
    "Service Name: fraud-detection-service\n",
    "\n",
    "Number of Tasks: 1 (Free Tier allows only 1)\n",
    "\n",
    "Networking:\n",
    "\n",
    "VPC & Subnets: Default is fine\n",
    "\n",
    "Security Group:\n",
    "\n",
    "Create a new one (e.g., fraud-detection-sg)\n",
    "\n",
    "Add rule: Allow TCP Port 5000 (or your app’s port)\n",
    "\n",
    "Load Balancer (Optional):\n",
    "\n",
    "If you want an API, attach an Application Load Balancer (ALB)\n",
    "\n",
    "Click Create Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36187f-8437-4c7b-aa70-1cb99f19f14d",
   "metadata": {},
   "source": [
    "### Option B: Deploying with AWS Lambda (Serverless)\n",
    "Lambda runs code without managing servers. Good for APIs, but has size limits.\n",
    "\n",
    "#### Step 1: Package Model for Lambda\n",
    "Lambda has a 250MB deployment package limit. If your model is large:\n",
    "\n",
    "- Use Lambda Layers (stores dependencies separately).\n",
    "\n",
    "- Or trim unnecessary files (e.g., remove unused libraries).\n",
    "\n",
    "1- Zip Your Lambda Code:\n",
    "\n",
    "- Your Python script (e.g., lambda_function.py)\n",
    "\n",
    "- requirements.txt (if using extra libraries)\n",
    "\n",
    "Example structure:\n",
    "\n",
    "fraud-detection-lambda/  \n",
    "├── lambda_function.py  \n",
    "├── requirements.txt  \n",
    "\n",
    "Zip it:\n",
    "\n",
    "cmd\n",
    "cd fraud-detection-lambda\n",
    "zip -r lambda_package.zip .\n",
    "\n",
    "2- Upload to S3 (Optional, if package is large):\n",
    "\n",
    "Go to AWS S3 Console\n",
    "\n",
    "Create a bucket → Upload lambda_package.zip\n",
    "\n",
    "#### Step 2: Create Lambda Function\n",
    "1- Go to AWS Lambda Console → Create Function\n",
    "\n",
    "2- Configure Function:\n",
    "\n",
    "Function Name: fraud-detection-lambda\n",
    "\n",
    "Runtime: Python 3.9\n",
    "\n",
    "Architecture: x86_64\n",
    "\n",
    "Permissions:\n",
    "\n",
    "Create a new role with basic Lambda permissions.\n",
    "\n",
    "3- Upload Code:\n",
    "\n",
    "Upload .zip file (or paste code inline if small).\n",
    "\n",
    "4- Set Memory & Timeout:\n",
    "\n",
    "Memory: 512MB\n",
    "\n",
    "Timeout: 1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49478fa4-2ea4-412a-bf2a-f5c1573cca36",
   "metadata": {},
   "source": [
    "#### Step 3: Set Up API Gateway (REST API for /predict)\n",
    "1- Go to API Gateway Console → Create API → REST API\n",
    "\n",
    "2- Configure API:\n",
    "\n",
    "- API Name: fraud-detection-api\n",
    "\n",
    "- Endpoint Type: Regional\n",
    "\n",
    "3- Create Resource & Method:\n",
    "\n",
    "- Click Actions → Create Resource → Name: predict\n",
    "\n",
    "- Click Actions → Create Method → POST\n",
    "\n",
    "4- Integrate with Lambda:\n",
    "\n",
    "Select your fraud-detection-lambda function.\n",
    "\n",
    "5- Deploy API:\n",
    "\n",
    "Click Actions → Deploy API\n",
    "\n",
    "Stage Name: prod\n",
    "\n",
    "✅ API is live! Test it at the provided URL (e.g., https://xxxx.execute-api.us-east-1.amazonaws.com/prod/predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f283db8-35fa-426f-81f3-e67d07e51aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
